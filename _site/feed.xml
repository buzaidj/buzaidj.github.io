<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-06-29T16:27:11-07:00</updated><id>http://localhost:4000/feed.xml</id><entry><title type="html">Do LA Metro bus stops have adequate shelter?</title><link href="http://localhost:4000/la-bus-stops" rel="alternate" type="text/html" title="Do LA Metro bus stops have adequate shelter?" /><published>2025-06-28T14:00:00-07:00</published><updated>2025-06-28T14:00:00-07:00</updated><id>http://localhost:4000/la_metro_bus_stops</id><content type="html" xml:base="http://localhost:4000/la-bus-stops"><![CDATA[<p>A few years ago, after reading about <a href="https://en.wikipedia.org/wiki/La_Sombrita">La Sombrita</a>, I wanted to get a good overview of LA bus stop amenities. I couldn’t find any statistics or data online, so I built a tool to gather data that picks a random stop from a list and presents a street view of the stop.</p>

<p><img src="/assets/rate_a_stop.png" alt="the rate a stop UX" width="500" /></p>

<p><a href="https://buzzy-bus-stops.web.app">Help me gather some data</a></p>

<p><a href="https://buzzy-bus-stops.web.app/summary">Download the data</a></p>

<h3 id="results-as-of-june-2025">Results (as of June 2025)</h3>
<p>I finally compiled the results a few years later. I sent the link to a few friends and have observations for around 175 unique bus stops. None of the four La Sombrita stops appear in the results (yet)! Of the 175 unique stops studied, reportedly <strong>25% have a shelter</strong>, 41% have shade, and 51% have seating. Even though 41% of stops had shade during observation, the shade structures in question seem to be buildings or trees, offering temporary shade depending on the time of day. So 41% could be an overestimate.</p>]]></content><author><name></name></author><category term="transit" /><summary type="html"><![CDATA[A few years ago, after reading about La Sombrita, I wanted to get a good overview of LA bus stop amenities. I couldn’t find any statistics or data online, so I built a tool to gather data that picks a random stop from a list and presents a street view of the stop.]]></summary></entry><entry><title type="html">I finished RTK! 2200 kanji later</title><link href="http://localhost:4000/rtk/" rel="alternate" type="text/html" title="I finished RTK! 2200 kanji later" /><published>2025-05-25T17:00:00-07:00</published><updated>2025-05-25T17:00:00-07:00</updated><id>http://localhost:4000/remembering-the-kanji</id><content type="html" xml:base="http://localhost:4000/rtk/"><![CDATA[<p>I finally finished James Heisig’s “Remembering the Kanji” (RTK). RTK is a somewhat heterodox approach of teaching the 2220 Jōyō Kanji 常用漢字. RTK is learning how to recall a kanji from a keyword by remembering radicals creating helpful mneomnics involving the Kanji. There’s no readings or vocabulary taught in the process.</p>

<h3 id="what-i-changed-from-the-stock-rtk">what I changed from the “stock” RTK</h3>

<p>RTK is hard. A lot of the keywords Heisig are synonyms. Upon review, if I ran into troubling synonyms, I decided that rather than pull my hair out, I’d just make the cards a little easier by adding one of the readings I knew to the card, or a longer definition.</p>

<div style="display: flex; flex-direction: row; justify-content: space-between; align-items: center">
    <div style="text-align: center">
        <img src="/assets/happiness_rtk.png" width="95%" />
        <div>my card for "happiness"</div>
    </div>
    <div style="text-align: center">
        <img src="/assets/rtk_箇.png" width="95%" />
        <div>my card for "箇"</div>
    </div>
</div>

<p>I also found it helpful to add radicals to the anki deck, something I didn’t see in other people’s decks. Many radicals Heisig uses are not Unicode characters and don’t render as anything logical, I found I could copy and paste a screenshot of the radical into the card, really helping here.</p>

<p><img src="/assets/rtk_grow_up_radical.png" alt="a card for the &quot;grow up&quot; radical" width="500" /></p>

<h3 id="what-i-did-wrong--learned-along-the-way">what I did wrong / learned along the way</h3>

<ul>
  <li>From kanji 1 to 30: I was convinced mnemonic were silly and I was too old for little stories. I was wrong.</li>
  <li>From kanji 1 to 100: I reviewed from kanji to keyword. This is wrong. Don’t do this. Heisig warns against this on several occasions. Production of a kanji from memory wires mental pathways in a way that simply remembering a keyword does not.</li>
  <li>From kanji 1 to 800: I didn’t set have the stroke order Anki extension set up on my deck. It’s helpful and reinforces stroke order <a href="https://ankiweb.net/shared/info/1964372878">link</a>. I’d recommend it.</li>
</ul>

<h3 id="the-routine">the routine</h3>

<p>Each day, I woke up around 6:30am to first review previously added kanji. I’d then go add 22 new Anki cards to my deck. Reviewing past cards and creating new cards took roughly an hour each, and on weekdays I’d promptly leave for work when finished around 8:30am. On average 22 new cards per day consisted of around 20 kanji after subtracting the non Jōyō (and often non-Unicode) radicals. The whole process took around four months, starting in late January and finishing in May.</p>

<p>When reviewing, I used pen and paper to practice stroke order and writing the kanji. I found writing the kanji out really helped commit it to memory. Covering every page in the notebook with kanji scribblings felt like an accomplishment.</p>

<p><img src="/assets/rtk_sample.jpg" alt="my notebook" width="500" /></p>

<h3 id="mnemonics">mnemonics</h3>

<p>These are really important. At first they seemed silly to me, but they help a ton with memory. I pulled inspiration from my personal life and often from <a href="https://hochanh.github.io/rtk/rtk1-v6/index.html">this site</a> with mnemonics. Creating clever mnemonics was fun. The sillier and the more bizarre the mnemonic, the more likely I was able to remember it.</p>

<h3 id="would-i-do-it-again">would I do it again?</h3>

<p>I don’t know. I feel like when I was deep in the RTK process, I was picking up and cementing previously known kanji really well. Now that I’m done with RTK my memory feels a little sluggish, and some characters don’t come back so well. Nonetheless, I feel like it did wonders for my visual recognition of kanji, but it’s hard to say I wouldn’t have the same or better result from spending the two hours I spent on RTK each day just reading a novel for example.</p>]]></content><author><name></name></author><category term="japanese" /><category term="language" /><category term="kanji" /><summary type="html"><![CDATA[I finally finished James Heisig’s “Remembering the Kanji” (RTK). RTK is a somewhat heterodox approach of teaching the 2220 Jōyō Kanji 常用漢字. RTK is learning how to recall a kanji from a keyword by remembering radicals creating helpful mneomnics involving the Kanji. There’s no readings or vocabulary taught in the process.]]></summary></entry><entry><title type="html">In Defense of Rent Control</title><link href="http://localhost:4000/rent-control" rel="alternate" type="text/html" title="In Defense of Rent Control" /><published>2024-10-18T23:10:00-07:00</published><updated>2024-10-18T23:10:00-07:00</updated><id>http://localhost:4000/in_defense_of_rent_control</id><content type="html" xml:base="http://localhost:4000/rent-control"><![CDATA[<p>Proposition 33 on the ballot this year in California repeals the 1995 Costa-Hawkins Rental Housing Act. I won’t go into details about the bill, but I wanted to defend rent control because I believe it is a red herring in the housing affordability debate.</p>

<p>There’s a game theory model to how much a landlord raises the rent on a tenant when the lease is up. The tenant can either stay or move at the end of the lease. If they stay they’ll be subject to the rent increase of the landlord. If they move they’ll be subject to the new rent plus the cost of finding and moving to a new unit. The landlord knows this and will capture the moving costs by raising the rent by a bit less than they think it costs the tenant to move units.</p>

<p>There’s a lot bundled into moving costs. Time spent searching for a unit, time taken off work touring units, the cost of hiring movers, and more. It’s particularly difficult for families who may have to move school districts and break routines. It’s also difficult for the elderly who have accrued emotional attachments to their unit and valuables and have mobility issues that make moving dreadful without outside help. Moving is expensive and difficult for a lot of people. Conniving landlords can take advantage of this and raise rents higher on tenants with higher moving costs.</p>

<p>Rent control policies help prevent this coercive dynamic. In San Francisco units in buildings built before 1979 are covered by the San Francisco Rent Ordinance. The ordinance rent is raised by a percentage of the Bay Area’s consumer price index. The current rate active from March 1, 2024 to February 28, 2025 is 60% of the Bay Area CPI. Buildings built after 1979 are not covered by the ordinance, including one of my friend’s buildings that tried raising his rent by 10% year over year.</p>

<p>Housing is expensive in California. We should not blame renters for this. Instead let’s focus on driving down the cost of housing in California. Efficiencies in permitting processes, denser zoning, and technological improvements in construction should make the cost of a housing unit fall.</p>

<p>Sources:</p>
<ol>
  <li><a href="https://www.sf.gov/learn-about-san-francisco-rental-laws">SF Gov - Learn About Rent Control Laws</a></li>
  <li><a href="https://www.sf.gov/news/annual-rent-increase-3124-22825-announced">SF Govt - Annual Rent Increases 3/1/24 - 2/28/25</a></li>
</ol>]]></content><author><name></name></author><category term="rent-control" /><category term="san-francisco" /><category term="housing" /><summary type="html"><![CDATA[Proposition 33 on the ballot this year in California repeals the 1995 Costa-Hawkins Rental Housing Act. I won’t go into details about the bill, but I wanted to defend rent control because I believe it is a red herring in the housing affordability debate.]]></summary></entry><entry><title type="html">Machine learning for detecting skate pops on a Teensy 4.1</title><link href="http://localhost:4000/teensy" rel="alternate" type="text/html" title="Machine learning for detecting skate pops on a Teensy 4.1" /><published>2024-10-11T10:33:00-07:00</published><updated>2024-10-11T10:33:00-07:00</updated><id>http://localhost:4000/machine_learning_teensy</id><content type="html" xml:base="http://localhost:4000/teensy"><![CDATA[<p>I live close to a skate park and skate there often. I wanted to build a system that could detect how busy the skate park near me is without actually recording anyone or tracking anyone. Video was out of the question, and the device would only operate on audio without the audio even leaving the edge device. I wanted to show that machine learning can be anonymous and compress data so much that its original form no longer exists.</p>

<p>I built a system that analyzes half-second chunks of audio to detect whether there are skateboard sounds or not (specifically skate “pops”). The training method is pretty extensible to other forms of audio. It uses a 33-dimensional feature vector based on Fourier transforms.</p>

<p>Predicted data would be sent back from that device to my house a block away, and a website would be updated with the current state of the park. I shelved the project for now due to the difficulty of getting the solar power and battery capacity to run the power-hungry Teensy 4.1 I’m using. I’ll pick it back up someday if there’s interest.</p>

<h2 id="data-collection">Data Collection</h2>

<p>The Teensy 4.1 has an SD card. I built a program for data collection that records WAV files onto the SD card. The device has two buttons: one for starting the recording (the black one with tape over it), and another (red mechanical keyboard switch) for holding during a skateboard sound. The data collector produces four files per audio recording: an audio file, a labels file with the beginning and ends of half-second chunks, a helper file with when the red button was pressed, and a file of feature vectors for each half-second of audio (<a href="https://github.com/buzaidj/skate-pops/blob/be8aab726e8b471adc390545297363209e8f0a56/teensy/teensy_program/teensy_program.ino#L509">code pointer</a>).</p>

<p><img src="/assets/teensy4_1.jpg" alt="Data collector (and predictor)" width="500" /></p>

<h2 id="feature-extraction">Feature Extraction</h2>

<p>I had to choose a method of feature extraction that would run efficiently on the Teensy. I did all the feature extraction/vectorization of audio on the Teensy even for training. This locked me into a certain set of features, but it made it far easier to know that my features were being generated the same between training and deployment.</p>

<p>I needed to generate vectors for half-second chunks. Buffering up a half-second of audio and running an FFT on that isn’t feasible due to memory constraints on the Teensy. Instead, smaller frames of audio are taken and aggregated together. Frames are 1764 samples long and overlap by 882 samples with their neighbors. This means there are 25 frames in a half-second chunk of audio.</p>

<p>Each frame is vectorized when it is received via FFT to compute <code class="language-plaintext highlighter-rouge">v_real</code> and <code class="language-plaintext highlighter-rouge">v_imag</code> of size 2048 (power of two above 1764). The sample rate is 44.1 kHz, meaning the max frequency observed by FFT is 22.05 kHz. The result in <code class="language-plaintext highlighter-rouge">v_real</code> is distributed evenly in the frequency domain up to half the sample rate. I bin that frequency into ten logarithmically scaled bins (<a href="https://github.com/buzaidj/skate-pops/blob/be8aab726e8b471adc390545297363209e8f0a56/teensy/teensy_program/teensy_program.ino#L867">code pointer</a>).</p>

<p>Two more features are added onto the vector. They are the root mean squared energy and the standard deviation of the root mean squared energy computed <a href="https://github.com/buzaidj/skate-pops/blob/be8aab726e8b471adc390545297363209e8f0a56/teensy/teensy_program/teensy_program.ino#L787">here</a>. Thus each frame has a vector of length 12.</p>

<p>After 25 frames are collected, they are aggregated into a dimension 33 vector as follows (<a href="https://github.com/buzaidj/skate-pops/blob/be8aab726e8b471adc390545297363209e8f0a56/teensy/teensy_program/teensy_program.ino#L1047">code pointer</a>). For each frequency bucket, the maximum of that bucket, the standard deviation of that bucket, and the mean of that bucket across the 25 frames are included. Additionally, the mean root mean squared energy and standard deviations of root mean squared energy across the 25 frames are included. Another feature, the “sum delta energies,” captures the L1 norm of variation in energy across frames, which may perform better because the skate pop sounds are sparse.</p>

<h2 id="data-labeling">Data Labeling</h2>

<p>Labeling solely requires marking the labels to 1 on the sections of audio where there are skateboard sounds. I used Audacity for this.</p>

<p><img src="/assets/labelling.png" alt="Labeling" width="500" /></p>

<h2 id="training">Training</h2>

<p>Feature vectors and labels are created in previous steps and maintained in a SQLite database. Training requires loading these vectors and labels, normalizing, and running backpropagation on the vectors for many epochs. Since there are far more 0 labels than 1 labels in the training dataset, I chose to use cross-entropy loss. The network has two hidden layers of 64 neurons and an output layer of two neurons where the higher probability neuron determines the predicted layer.</p>

<p><a href="https://github.com/buzaidj/skate-pops/blob/be8aab726e8b471adc390545297363209e8f0a56/teensy/data_labelled/nn_model.py#L61">Training code</a></p>

<h2 id="deploying-the-model">Deploying the Model</h2>

<p>I recreated the model in C++ on the device via exporting the weights to a <a href="https://github.com/buzaidj/skate-pops/blob/be8aab726e8b471adc390545297363209e8f0a56/teensy/teensy_program/nn_model_params.h">.h file</a> that gets deployed onto the Teensy. I wrote an <a href="https://github.com/buzaidj/skate-pops/blob/be8aab726e8b471adc390545297363209e8f0a56/teensy/teensy_program/teensy_program.ino#L113">implementation</a> of a forward pass function on the Teensy as well.</p>

<p>Here’s it in action trained on me tapping my pen:</p>

<video controls="" width="300">
  <source src="/assets/detector.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>]]></content><author><name></name></author><category term="ml" /><category term="teensy" /><category term="microcontroller" /><summary type="html"><![CDATA[I live close to a skate park and skate there often. I wanted to build a system that could detect how busy the skate park near me is without actually recording anyone or tracking anyone. Video was out of the question, and the device would only operate on audio without the audio even leaving the edge device. I wanted to show that machine learning can be anonymous and compress data so much that its original form no longer exists.]]></summary></entry><entry><title type="html">T9 keyboards and spell-checking</title><link href="http://localhost:4000/better-t9/" rel="alternate" type="text/html" title="T9 keyboards and spell-checking" /><published>2024-10-11T08:05:00-07:00</published><updated>2024-10-11T08:05:00-07:00</updated><id>http://localhost:4000/better-t9-ideas</id><content type="html" xml:base="http://localhost:4000/better-t9/"><![CDATA[<div style="display: flex; flex-direction: row; justify-content: space-between; align-items: center">
    <div style="text-align: center">
        <img src="/assets/cat-s22.jpg" width="200" />
        <div>Cat S22</div>
    </div>
    <div style="text-align: center">
        <img src="/assets/t9.png" width="100" />
        <div>T9 Keyboard</div>
    </div>
    <div style="text-align: center">
        <img src="/assets/t9-misspelled-word.png" width="200" />
        <div>"Abreviations"</div>
    </div>
</div>

<p>The CAT S22 is a $65 flip phone that runs Android. It has a numpad and a small touch screen. I’ve been using it as my primary phone for the past few months. The screen is so small it’s hard to do anything else other than texting, calling, and some basic web surfing. Which is why I bought it.</p>

<p>Although the phone has a numpad, it doesn’t come stock with a T9 keyboard application. T9 is a predictive text technology for mobile phones with numeric keypards that makes texting easier. Typing “43556” for example produces “hello”. Becasue numeric strings don’t uniquely map to words, after typing the numbers one clicks left/right with the D-pad to select their desired word.</p>

<p>I downloaded the TT9 app which is great and has all the core T9 functionality. The only issue I’ve run into is that TT9’s implementation, like most, isn’t very robust to small spelling errors. For example spelling “abbreviations” with one “b” goes horribly wrong. Currently, the numbers you type must all have some mapping to the letters in the word you want. Missing a number, adding an additional number, or mistyping one number for another results in the desired word not appearing in the result set and the user having to start typing the word over again.</p>

<p><img src="/assets/t9_db_example.png" alt="t9 example" width="500" /></p>

<p>TT9 uses a SQLite database with T9 sequences and words like the one in the image above and upon each key press queries the dictionary for that sequence. Querying the dictionary produces the set of words that start with that sequence subject to ordering by word length and frequency and a limit. There doesn’t appear to be much caching of results from previous queries (e.g. going from hel -&gt; hell -&gt; seems to requery the database) meaning keeping queries short is pretty important.</p>

<p>One approach to adding “spell correction” is adding all results with edit distance 1 to the result set. We can substitue a character, insert a character in any position, or delete any character. For a five character word we’d have to make an additional 110 queries. The number of queries made grows quadratically with sequence length. This seems infeasible due to the exploding number of queries we’ll need to make.</p>

<p>Another simple approach is to just “generate common mispellings” of words, thereby pushing the explosion on the side of the database rather than the number of queries made.</p>]]></content><author><name></name></author><category term="t9" /><category term="keyboard" /><category term="ml" /><category term="algorithms" /><category term="data-structures" /><summary type="html"><![CDATA[Cat S22 T9 Keyboard "Abreviations"]]></summary></entry><entry><title type="html">AliExpress LoRa Transceivers</title><link href="http://localhost:4000/lora-transceivers/" rel="alternate" type="text/html" title="AliExpress LoRa Transceivers" /><published>2024-08-14T18:07:16-07:00</published><updated>2024-08-14T18:07:16-07:00</updated><id>http://localhost:4000/lora</id><content type="html" xml:base="http://localhost:4000/lora-transceivers/"><![CDATA[<p><img src="/assets/lora.jpg" alt="LoRa Node" width="500" /></p>

<p>I’m using a set of $7 E32-900T20D LoRa transceivers to send data back from an edge compute machine learning device about 500 ft from my house at a rate of about a byte per second.</p>

<p>LoRa is a digital proprietary spread spectrum radio communication technique designed for low power transmitters on license-free frequency bands. In the United States, the 902 - 928 MHz band is most frequently used. Currently, the <a href="https://news.ycombinator.com/item?id=41226802">FCC is seeking comments</a> on re-assigning parts of the band to NextNav.</p>

<p>The module by default is configured to around 870 MHz, which is illegal to transmit on unlicensed in the United States. You’ll have to change the transmit frequency to something in the 902 to 928 MHz range. This requires putting the module to sleep using the M0 and M1 pins, sending a command to set the parameters, and waking it back up to transmit/receive.</p>

<p>There’s no terminator byte (\0) that indicates a command or message is over. The module just waits until enough time has gone by before terminating the message. Sending commands or messages over <code class="language-plaintext highlighter-rouge">Serial</code> will involve blocking to ensure the message is ready to send another message.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>void sendMessage(uint8_t* message, uint8_t len) {
    LoraSerial.write(message, std::max(len, 55));
    delay(50);
}
</code></pre></div></div>

<p>I found the existing Arduino module written for this library a bit difficult to use, so I wrote a pretty <a href="https://github.com/buzaidj/ping-pong-e32/blob/main/E32Driver.cpp">constrained one for my use case</a>.</p>]]></content><author><name></name></author><category term="lora" /><category term="915mhz" /><summary type="html"><![CDATA[]]></summary></entry></feed>